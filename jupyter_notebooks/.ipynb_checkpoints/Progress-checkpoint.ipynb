{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6/13\n",
    "\n",
    "Cosine similarity metric is able to separate some clusters but not all. For a general data set, identification of important clusters may not occur.\n",
    "\n",
    "Example: A few of the major groups were found almost immediately but, a specific region is not broken down until over 60 clusters are calculated.\n",
    "\n",
    "Ideally cluster similarity should be measured by looking at peaks and their positions.\n",
    "\n",
    "Another consideration for runtime and the clusters that are being formed is giving the clustering algorithm an adjacency matrix to represent the connections between adjacent cells. This will adjust how clusters are formed and also may improve runtime.\n",
    "\n",
    "![Cosine Clustering](images/clusters-3-10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6/14\n",
    "\n",
    "Implementing a phase coloring based on relative similarity. The goal is to see major colors like red, green, blue in the major clusters and then as clusters split the parts retain the colors of the parent cluster. This should result in a plot where the changing colors show the gradients in cosine similarity and regions that have a single material/spectra will have a single almost unchanged color.\n",
    "\n",
    "Initial implementation:\n",
    "Start with 3 clusters and assign then the colors red, green, blue. Then proceed to split clusters based on the hierarchical clustering method. Each time a cluster is split the the neighbors in the color wheel N1, N2 and the two children clusters C1, C2 are considered.\n",
    "\n",
    "The children's colors are assigned such that the hue difference between N1 - C1 - C2 - N2 is proportional to the similarity of the pairs. The order of C1,C2 (N1 - C1 - C2 - N2 vs. N1 - C2 - C1 - N2) is determined by the similarity of C1 and C2 to N1. The more similar pair is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6/17\n",
    "\n",
    "Wafer coloring implemented with following results. Because of the use of the cosine the clusters of interest are not segmented. The general region of the cluster is found but the specific points are not identifiable. Furthermore, there is a clear split in shade through the desired cluster.\n",
    "\n",
    "![177 Cosine Clusters](images/Cosine_177.png)\n",
    "\n",
    "\n",
    "Plan is to adjust the coloring to be on a linear scale. Ideally this would mean that red and green colors are the furthest from each other and the rest are some combination. Also when a cluster is split all other colors are shifted based on the similarity of the corresponding clusters. Instead of just considering the two neighbors and scaling the split in the range between them, the entire hue spectra is considered and all the clusters so far mapped are placed proportionally to the similarity of their neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6/18\n",
    "\n",
    "Implementation of colormap with linear color scale resulted in a less informative visual. Because all of the similarity values are rescaled at each split the cluster colors become an even distribution over the color spectrum. Even though the separation is proportional to similarity and similarity is scaled (subtracting min value) the majority of the similarity values are the same so the scale is almost uniform.\n",
    "\n",
    "![177 Linear Cosine Clustering](images/Cosine_linear_177.png)\n",
    "\n",
    "Either way, because of the large number of points there is almost so visible gradient anywhere in the image that corresponds to any desired phases. Instead the visual produced shows the order in which grid locations are added to clusters in a build up clustering approach. This does produce a spiral pattern but is not really the information we are trying to visualize.\n",
    "\n",
    "The plan is to transition to using peaks or some form of peak metric to measure similarity.\n",
    "\n",
    "Peak location will be done via Roberts code. (how to merge the code / what files are needed)\n",
    "\n",
    "Another task is to write a container for the existing code allowing it to function independently of the system. Plan is to start with Singularity and potentially, if need be, use Docker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6/19\n",
    "\n",
    "For completeness L1 and L2 distance visuals have been implemented to see how well these similarity metrics can differentiate between clusters. Results are similar to using the cosine metric with some variations. No clear advantages to either method.\n",
    "\n",
    "![L1](images/L1_177.png)\n",
    "![L2](images/L2_177.png)\n",
    "\n",
    "Code for locating peaks in spectra data needs to be streamlined into the current clustering algorithm and then the effectiveness of the method can be assessed.\n",
    "\n",
    "A container to run all exiting code has been implemented. It automatically installs the latest version of the code (from github) and comes with the TiNiSn_500 and TiNiSn_600 data. Testing the container to make sure it runs on other machines is still necessary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6/20\n",
    "\n",
    "Peak based clustering implemented with significant results. Plotting the number of peaks in each spectra results in a limilar plot to L1 and L2 similarity above. Computing a similarity of two spectra based on the number of peaks not present in the other results in a plot where key regions are identifiable. To determine if a peak is present in both plots a \"delta\" parameter is used as the maximum allowed peak shift. Withing this delta two peaks are considered to be the same. This value was picked based on the resulting plots so a method to properly determine the value before hand is necesary. Furthermore, many clusters that should be whole are split up by this method. Potentially incorporating the peak intensities can improve the algorithms performance.\n",
    "\n",
    "![Peak Clustering](images/peak_clust-0.049-31.png)\n",
    "\n",
    "Note how the Heusler and half-Heusler regions are bounded along one of their sides while in the other direction the region extends further than necesary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6/21\n",
    "\n",
    "Presentation: https://docs.google.com/presentation/d/1LDtxghLUUXl52NSUOivJ78-jeY5iKSavsgqMwsCn7AY/edit#slide=id.g5c0eef7190_0_88\n",
    "\n",
    "The new direction is to identify peak clusters (the regions in space that correspond to the peaks) and use these to represent diffraction patterns.\n",
    "\n",
    "For example: across all the diffraction patterns some 7 unique peaks are found (including peak shifting) then the reduced representation may look like (0,0,0.1,3,0,0,5) where 0 corresponds to not having that peak and the values correspond to the presence and intensity of the peak.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6/24\n",
    "\n",
    "Peak Clustering has been implemented with some results. All the peaks in the data set are mapped to coordinates based on their x,y position in the grid and the position of the peak in the diffraction pattern (p). Each peak is assigned the coordinate (x/100,y/100,p). The x,y coordinates are scaled to prevent clustering in the peak axis instead of the x,y axes.\n",
    "\n",
    "Peaks are clustered into 50 groups. This is an arbitrary value and needs to be adjusted to match the true number of peaks.\n",
    "\n",
    "Peak clustering is used to convert each diffraction pattern into 50 dimensional vectors (called peak vectors). These vectors are then used to cluster the diffraction patterns on the wafer. This produces the following clusterings:\n",
    "\n",
    "![PeakReductionClustering](images/PeakReductionClustering_50.png)\n",
    "\n",
    "Clustering seems to correspond more accurately to the actual peaks in the data set, although some balancing of peak intensity versus peak presence needs to be made.\n",
    "\n",
    "\n",
    "![PeakReductionClustering](images/PeakReductionClustering_60.png)\n",
    "\n",
    "Here with 60 different possible peaks the clustering is more broad (regions are rounder) and it seems to align better with the actual clustering. The Orange cluster is present when with 50 peak clusters it was not.\n",
    "\n",
    "With some analysis of how peaks are clustered it seems like around 58 peak clusters is where the same peak is split into two labels so 57 peak clusters is used:\n",
    "\n",
    "\n",
    "![PeakReductionClustering](images/PeakReductionClustering_57.png)\n",
    "\n",
    "Here we can see the amorphous region, half Heusler, and full Heusler decently separated from the rest of the wafer. However some individual grid locations still pertain to the mentioned clusters when they shouldn't. This seems to be a product of the current similarity function.\n",
    "\n",
    "The similarity between two diffraction patters is found as follows. The peak vectors are obtained for each. The values in these vectors are log scaled and the L1 distance is taken. Clustering is performed without connectivity.\n",
    "\n",
    "A specific example of how clustering goes wrong. Above between the blue and purple clusters in the lower region of the wafer. The vertical split should actually be slanted as seen from the plots below.\n",
    "\n",
    "![DF_Plot_1](images/DF_Plot_1.png)\n",
    "\n",
    "\n",
    "Here is an example of where one group of peaks is split into multiple labels. Note the 2 vs 52 labels.\n",
    "\n",
    "![DF_Plot_2](images/DF_Plot_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6/25\n",
    "\n",
    "Peak Clustering analyzed. Seems that the clusters being found are not completely what is desired. \n",
    "https://docs.google.com/presentation/d/1LbSoimvhLF_YgP4nXWMb8xxLKqh7yunrSV6v92U5Cg4/edit?usp=sharing\n",
    "\n",
    "Plan is to normalize peaks before clustering them. Then use a single parameter to scale the p-axis to determine idea value for labeling peaks. Also, adjust sensitivity in peak finding code to locate more significant peaks. Finally, plot peak width and peak count gradients along with clustering to see if there are significant differences that are being missed.\n",
    "\n",
    "Additional Directions\n",
    " - Different peak finding approaches\n",
    " - PCA on peak vectors to identify independent clusters\n",
    " - Different clustering approaches to count the number of clusters\n",
    " - Similarity Metric can be adjusted to incorporate peak width/other parameters\n",
    " - Similarity metric can be adjusted to put more weight on new peaks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6/26\n",
    "\n",
    "Peak points have been normalized for clustering and the idea scale parameter for the p-axis seems to be around 30-100. Larger scales make the different \"peak plains\" more distinct but risks seperating planes that are slanted.\n",
    "\n",
    "Analysis of clustering methods without n_clusters\n",
    "\n",
    "DBSCAN (almost perfect)\n",
    "\n",
    "![DBSCAN_Peak_Clustering](images/DBSCAN_Peak_Clustering.png)\n",
    "\n",
    "\n",
    "OPTICS\n",
    "\n",
    "![OPTICS_Peak_Clustering](images/OPTICS_Peak_Clustering.png)\n",
    "\n",
    "BIRCH\n",
    "\n",
    "![BIRCH_Peak_Clustering](images/BIRCH_Peak_Clustering.png)\n",
    "\n",
    "From this analysis it seems that DBSCAN (with good parameters can achieve similar clustering to Agglomerative (which was used before).\n",
    "\n",
    "Using DBSCAN peak clustering and hierarchical clustering with L1 similarity the following cluster map is produced. Some clusters are reasonable but adjustments to the similarity metric need to be made to place more value on new peaks.\n",
    "\n",
    "![DBSCAN_Cluster_Plot](images/DBSCAN_Cluster_Plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6/27\n",
    "\n",
    "\n",
    "PCA clustering implemented on top of the existing dimentionality reduction with DBSCAN. PCA is used to further reduce the diffraction patterns to 20 components and then agglomerative clustering is used to produce the clusters (below).\n",
    "\n",
    "![DBSCAN_Peaks_Width](images/DBSCAN_Peaks_Width.png)\n",
    "\n",
    "PCA does not seem to provide significant improvements to the produced clustering when compared to the same plot without PCA reduction (below). The second plot above shows the number of peaks relative to the maximum in each cluster. This plot does have some indication to the position of the half Heusler but, it also means that the clustering technique is again missing an entire phase. On the right there is a plot of the largest peak width in each cluster. It shows that there is an amorphous region in the top left. (white corresponds to Nan values in the data for the peak width).\n",
    "\n",
    "To understand why these new methods don't seem to be improving the results we can plot the unique \"peak vectors\" for each diffraction patters ie which peaks are present/not present for each diffraction pattern.\n",
    "\n",
    "![unique_peak_vector_plot](images/unique_peak_vector_plot.png)\n",
    "\n",
    "Here we can see that the majority of the diffraction patterns in the wafer are unique with respect to the peaks being found. This also holds true for different amounts of sensitivity in the peak finding code. This is either because the peak clustering is failing or because the peak finding method is missing/finding extra peaks. The following is an example of how some peaks are not being detected.\n",
    "\n",
    "![Missing_Peaks](images/Missing_Peaks.png)\n",
    "\n",
    "Notice that on the red curve, just before 3.0 and 4.25, there are secondary peaks that are merged with larger ones. These peaks are important but are not being detected. Also notice the peak on the green curve near 4.5. This peak is not detected. Adjusting the sensitivity may find this peak but then other noise in the data which may not be a peak can still be classified. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6/28\n",
    "\n",
    "Meeting: https://docs.google.com/presentation/d/1W0IAJGbA_Cv5EFLt1ppfoBdD9oUBmTpz7X25ejGXYyI/edit?usp=sharing\n",
    "\n",
    "Peak finding code needs to be modified to better detect peaks. Specifically partially merged peaks as shown above (6/27). Running peak detection on the square root of the diffraction pattern may improve detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7/1\n",
    "\n",
    "Peak count and width penalty scores implemented to show cluster membership. Additionally, the combined penalty over all clusters is plotted as the number of clusters increases.\n",
    "\n",
    "![PenaltyScorePlot](images/PenaltyScorePlot.png)\n",
    "\n",
    "The top left plot shows the clustering(everypoint is its own cluster)\n",
    "The top middle shows the peak count penalty in the gradient (darker is higher penalty, again all fully bright)\n",
    "The top right shows the peak width penalty score (solid color as every point is its own cluster)\n",
    "\n",
    "The bottom plot shows the combined peak count and peak width penalties over time.\n",
    "\n",
    "The peak width penalty doesn't seem to hold much indication on the correct number of clusters. Furthermore, the peak penalty, after 20 clusters, is further improved by each additional cluster. There seems to be an initial bump in the peak penalty which may be due to \"unlucky\" cluster division.\n",
    "\n",
    "Aditional plot at 20 clusters:\n",
    "\n",
    "![PenaltyScorePlot2](images/PenaltyScorePlot2.png)\n",
    "\n",
    "\n",
    "First approach to improving the peak finding algorithm is to sqrt the peak intensities before running peak detection. This allows smaller peaks to be more visible.\n",
    "\n",
    "![PenaltyScorePlotSqrt](images/PenaltyScorePlotSqrt.png)\n",
    "\n",
    "It seems that this change also divides one of the clusters known to be single phase. This means that either this method detects too many peaks (ie extra peaks that arent in the single phase region) or it has only detected a portion of the peaks that need to be found in the single phase region and so splitting it.\n",
    "\n",
    "\n",
    "Below is a video version of these plot to demonstrate how dividing clusters affects the penalties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"320\" height=\"240\" controls>\n",
       "  <source src=\"images/peak_width_penalty.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<video width=\"320\" height=\"240\" controls>\n",
    "  <source src=\"images/peak_width_penalty.mp4\" type=\"video/mp4\">\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7/2\n",
    "\n",
    "Evaluation of the video above shows that the single phase regions persist as more clusters are created. A future direction is to automatically detect these \"persistant\" regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7/3\n",
    "\n",
    "Peak finding mistakes evaluated and recorded through manual process.\n",
    "\n",
    "![PeakFindingErrors](images/PeakFindingErrors.png)\n",
    "\n",
    "Note: some peaks are missed even in manual process.\n",
    "\n",
    "Plan is to evaluate these errors and see how they can be detected. Peak finding on the log or sqrt of the data may improve visibility of the peaks. Or a different approach can be used to find more peaks.\n",
    "\n",
    "A criteria can be determined for the peaks that are missed and this can help identify how the algorithm needs to be adjusted.\n",
    "\n",
    "Also clustering in the \"heat map\" of the peaks may help with detection.\n",
    "\n",
    "![peak_heat_map](images/peak_heat_map.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7/5\n",
    "\n",
    "Peak error plot:\n",
    "\n",
    "![peak_error_plot](images/peak_error_plot.png)\n",
    "\n",
    "Black dots are local maxima, green are local minima, and red is the manually labeled mistake.\n",
    "The plot titles are the vertical distances between the labeled error and the nearest local minima.\n",
    "\n",
    "Figure produced with proportion measurements. The black dot is the nearest peak found by the algorithm. The red dot as before is a missed peak and the green dot is a local minima. The title of each plot is the proportion that the mistake stands out from the local minima relative to the peak.\n",
    "\n",
    "Proportion = (Error - min) / (Peak - min)\n",
    "\n",
    "![peak_error_proportion_plot](images/peak_error_proportion_plot.png)\n",
    "\n",
    "Notice that the proportions vary significantly and are relatively small. Note in some sholder cases the minimum is not between the peak and the error (here the proportions holds less meaning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7/9\n",
    "\n",
    "Work towards density based clustering in heat map. In order for the peak heat map seen below to be \"clustered\" by existing algorithms it needs to be converted to a collection of points.\n",
    "\n",
    "To do this, every pixel in the image is filled uniformly with random points. The number of points corresponds to the brightness.\n",
    "\n",
    "Now a density based clustering method can be used to cluster on the heat map.\n",
    "\n",
    "![peak_heat_map](images/peak_heat_map.png)\n",
    "\n",
    "The following images are the results of using several clustering methods:\n",
    "\n",
    "DBSCAN\n",
    "\n",
    "![heat_map_DBSCAN](images/heat_map_DBSCAN.png)\n",
    "\n",
    "GMM\n",
    "\n",
    "![heat_map_GMM](images/heat_map_GMM.png)\n",
    "\n",
    "Agglomerative (single linkage)\n",
    "\n",
    "![heat_map_Agg](images/heat_map_Agg.png)\n",
    "\n",
    "DBSCAN is able to grab major clusters but, fails around the edges. A lot of small regions that aren't clusters are found.\n",
    "\n",
    "GMM is able to make divisions between large clusters but, it dedicates too many points to each cluster. This is an artifact of how the algorithm works.\n",
    "\n",
    "Agglomerative effectively grabs the clusters but the one on the right is not properly split.\n",
    "\n",
    "\n",
    "\n",
    "General points:\n",
    "\n",
    "It may be better to uniformly distribute points in the grid regions. The random nature of the current method may be creating high density spots in low density regions and so creating outlier points seen in DBSCAN. Furthermore, there may be other random events that further make clustering more difficult.\n",
    "\n",
    "Potentially a custom clustering algorithm could be made to cluster based on the heat map itself. This can avoid the problem of representing the heat map as a collection of points and can provide a more rigorous method of clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7/11\n",
    "\n",
    "Peak finding code fixed to report curves that are fitted to data. The found peaks are plotted in junction with peaks not found by the method earlier.\n",
    "\n",
    "\n",
    "![peak_error_proportion_plot_fixed](images/peak_error_proportion_plot_fixed.png)\n",
    "\n",
    "Black - peaks found by code\n",
    "Red - peaks missed by earlier algorithm\n",
    "Green - local min\n",
    "\n",
    "Notice that many of the peaks that were missed before are now properly found. However, there are now false positive reports of peaks.\n",
    "\n",
    "\n",
    "Work started on different peak fitting algorithm. The idea is to use local minima to select the blocks instead of BBA. This is more likely to create regions that don't cut peaks into parts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7/12\n",
    "\n",
    "Initial peak fitting implemented. Sholder peaks are not detected unless they are a local maxima. The advantage of this method is that many more small peaks are detected.\n",
    "\n",
    "![min_block_peak_fitting](images/min_block_peak_fitting.png)\n",
    "\n",
    "![min_block_peak_fitting_errors](images/min_block_peak_fitting_errors.png)\n",
    "\n",
    "Notice that in the image above two separate sholder peaks are completely missed by this method. This is because they are not bound by a local minima. Additionally there is a peak on the right side which is \n",
    "\n",
    "Additionally peaks need to be plotted on the peak heat map for visualization and peak_width_penalty, DBSCAN_PCA_AGG, need to be rerun with fixed peakBBA method and new method for comparisons.\n",
    "\n",
    "\n",
    "Analysis of min-block peak fitting and BBA curve fitting:\n",
    "\n",
    "The black dots are the peaks found by the BBA curve fitting algorithm.\n",
    "The black X'sare the peaks found by the min-block curve fitting method.\n",
    "\n",
    "![minblock_BBA_curves_analysis1](images/minblock_BBA_curves_analysis1.png)\n",
    "\n",
    "Here we see that the BBA curve fitting algorithm has some false positives and doesn't always hit peaks excactly.\n",
    "The min-block curve fitting method seems to grab more peaks but, grabs extra peaks which are very small. It is not clear if they are noise. The two methods will be compared in terms of how they produce clusters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7/15\n",
    "\n",
    "Peaks on Heat Map Plot \n",
    "\n",
    "Peak fitting using local minima as blocks:\n",
    "![peaks_on_heatmap_minblock](images/peaks_on_heatmap_minblock.png)\n",
    "\n",
    "Peak fitting using BBA algorithm to find blocks. Local max of each section.\n",
    "![peaks_on_heatmap_BBApeaks](images/peaks_on_heatmap_BBApeaks.png)\n",
    "\n",
    "Peak fitting using BBA algorithm to find blocks. Centers of fitted curves to each block.\n",
    "![peaks_on_heatmap_BBAcurves](images/peaks_on_heatmap_BBAcurves.png)\n",
    "\n",
    "\n",
    "Clustering (DBSCAN_PCA_AGG) redone with peak BBA, curve BBA, and peak min-block\n",
    "\n",
    "Min-Block peak fitting: Ignoring the white plot it is visible that due to the many extraneous peaks the algorithm is unable to properly seperate clusters.\n",
    "![DBSCAN_PCA_AGG_minblock_peaks](images/DBSCAN_PCA_AGG_minblock_peaks.png)\n",
    "\n",
    "BBA peak fitting: (same algorithm as originally implemented) Clustering is decent but, not perfect.\n",
    "![DBSCAN_PCA_AGG_BBA_peaks](images/DBSCAN_PCA_AGG_BBA_peaks.png)\n",
    "\n",
    "\n",
    "BBA curve fitting: Clustering seems to much more acurately grab clusters in the wafer.\n",
    "![DBSCAN_PCA_AGG_BBA_curves](images/DBSCAN_PCA_AGG_BBA_curves.png)\n",
    "\n",
    "In all three cases the mle solver for pca determined that only 1 dimension needed to be reduced from the initial peak dimention reduction. This may be because of the occasional false positives in the peak finding code.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7/16\n",
    "\n",
    "Work started on Cluster Analysis GUI.\n",
    "\n",
    "Gui so far includes:\n",
    "- data and peak parameter loading\n",
    "- custom regex (for file selection)\n",
    "- cosine / peak clustering selection\n",
    "- Cosine clustering plot\n",
    "\n",
    "Additional features:\n",
    "- selecting wafer locations to plot\n",
    "- switching to peak clustering mode\n",
    "- switching between types of plots in each mode \n",
    "    - Cosine membership scale\n",
    "    - peak width\n",
    "    - peak count\n",
    "    - Cluster center plot\n",
    "- switching between numbers of clusters\n",
    "- plotting N and N+1 clusters side by side with matching colors\n",
    "- saving plots as png files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7/22\n",
    "\n",
    "Gui wrapped up to usable level. Still more features to implement and internal coding fo fix.\n",
    "\n",
    "features:\n",
    "- plot peak data\n",
    "- plot fitted curves\n",
    "- peak count plot with curve params or peak params option\n",
    "Internal\n",
    "- all windows formed through new window (less duplicate code)\n",
    "- clustering methods in a new class\n",
    "- different grid types\n",
    "- loading data in arbitrary format/ indicate required format\n",
    "\n",
    "\n",
    "Plan for identifying background noise in data.\n",
    "\n",
    "During data collection a part of the background can intentionally be grabbed in order to measure the background noise. The background is fitted with a cosine curve and the residual is computed relative to the fit. The standard deviation of the residual times 3 gives the bounds on the noise for the data.\n",
    "\n",
    "![noise_interval](images/noise_interval.png)\n",
    "\n",
    "A visual of the curve that is being fit to the background region\n",
    "\n",
    "![noise_interval_curve](images/noise_interval_curve.png)\n",
    "\n",
    "Here there are some example peaks that are larger and smaller than the noise range. The method for filtering these still needs to be determined.\n",
    "\n",
    "![noise_interval_zoom](images/noise_interval_zoom.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7/23\n",
    "\n",
    "Peak BBA code modified to use local minima as change points. This results in fewer peaks being found by the algorithm. This is likely due to the cutoff for peak height and some differences in implementation of the peak fitting code.\n",
    "\n",
    "Original heatplot with peaks in the peak param file.\n",
    "\n",
    "![peakBBA_heatplot](images/peakBBA_heatplot.png)\n",
    "\n",
    "heatplot with peak params generated using local minima as change points.\n",
    "\n",
    "![peakMin_heatplot](images/peakMin_heatplot.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7/24\n",
    "\n",
    "Removing peaks with amplitude below threshold proves unsuccesful. Because of how the curves are fit certain peaks with very small amplitudes are actually fit by very large peaks to match the curve. This can be seen on the image below.\n",
    "\n",
    "![background_MinBlockPeak_removal](images/background_MinBlockPeak_removal.png)\n",
    "\n",
    "A similar issue arrises with the curveparam files but it is instead because of how the algorithm fits the peaks. (by subtracting out fittied peaks) When two curves are fit to a single peak the curve with an insignificant amplitude may also be the curve that marks the actual peak. The result is an odd removal of peaks.\n",
    "\n",
    "![background_BBAcurve_removal](images/background_BBAcurve_removal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8/14\n",
    "\n",
    "Itterative curve fitting implemented. Blocks are chosen based on local minima below median. This prevents double peaks from being split. Also blocks smaller that 20 data points are merged into neighbors for better fitting.\n",
    "\n",
    "Curve fitting is limited to four curves. Curve fitting stops when residual has fewer than 2 statistically significant points (2 SD).\n",
    "\n",
    "Key issues:\n",
    "Misses sholder peaks (single curve fits pretty well)\n",
    "Detects small noise peaks (can be manually removed)\n",
    "Detects non-existant peaks (asymetry in peaks)\n",
    "Skips peaks (some peaks / peak clusters ar not \"sharp\" enough to not be noise)\n",
    "\n",
    "Examples\n",
    "![itterative_curve_fitting_sholder_peak](images/itterative_curve_fitting_sholder_peak.png)\n",
    "![itterative_curve_fitting_noise_peaks](images/itterative_curve_fitting_noise_peaks.png)\n",
    "![itterative_curve_fitting_non_peaks](images/itterative_curve_fitting_non_peaks.png)\n",
    "![itterative_curve_fitting_missing_peak](images/itterative_curve_fitting_missing_peak.png)\n",
    "\n",
    "Notice: Red lines indicate block boundries.\n",
    "\n",
    "Constants:\n",
    "-peak alpha and gamma parameters are bounded from 0 to .03 This allows for broad peaks to mask some background but peaks are still forced to be mostly sharp. This may cause someof the bad fits as the initially fit sharp curve cn make the residual look like noise.\n",
    "\n",
    "-blocks smaller than 20 data points are merged. This just seems to produce better blocking.\n",
    "\n",
    "\n",
    "\n",
    "Noise detection:\n",
    "\n",
    "attemps:\n",
    "comparing residual to random noise with scipy.stats.ks_2samp()\n",
    " - overall bad predictions\n",
    "statsmodels.stats.diagnostic.acorr_ljungbox to get noise measurement\n",
    " - output doesn't seems to indicate noise\n",
    " \n",
    "Future direction: hywavwn.test()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8/15\n",
    "\n",
    "Constant noise threshold implemented. To check if residual is equivalent to noise it's maximum displacement is compared to a constant term.\n",
    "\n",
    "The result is a better overall fit. No peaks are missed. Occasionally finds extra peaks.\n",
    "\n",
    "Image of fit:\n",
    "\n",
    "![itterative_curve_fitting_constant_noise_thresh](images/itterative_curve_fitting_constant_noise_thresh.png)\n",
    "\n",
    "Black - data\n",
    "Blue - fit\n",
    "Green - residual\n",
    "Orange - individual curves\n",
    "Red - blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8/16\n",
    "\n",
    "Constant noise threshold tested with a calculated background noise. This is found to be inconsistent. \n",
    "\n",
    "A cosine curve is fitted to a region of data assumed to be the background. Tripple standard deviation of the residual of the fit is used as the threshold.\n",
    "\n",
    "This tends to be unreliable because the calculated threshold ranges from values too small (less that 3) to values too big (greater that 20). In these cases either no fit is found or peaks are missed respectively.\n",
    "\n",
    "The data in the range 1.5 to 1.8 was used.\n",
    "\n",
    "\n",
    "Improvement to fitting algorithm through better guess curves. The initial guess curve is optimized with alpha and gamma parameters from 0 to 2 (a sharper peak). If this fails then the parameters are widened to 0 to 5 and then to 2 to 5. Initially a sharp peak is fit because this is what the majority of the guess curves look like. Then when that fails then wide peaks are fitted. Keeping a smaller range allows for faster convergence.\n",
    "\n",
    "There should not be peaks with parameter values larger than 2 (approximately) as these are too wide. These tend to be background \"peaks\" and can later be filtered out.\n",
    "\n",
    "Run time analysis:\n",
    "TiNiSn_500C Data set time per diffraction pattern without optimization: \n",
    " - 18.37 seconds average\n",
    " - 8.85 seconds average removing outlier (#63)\n",
    " - outlier: 28 minutes\n",
    " - 5.6 seconds median\n",
    " \n",
    "\n",
    "The outlier due to a oddly shapped peak (first two clear peaks). Because of the odd shape the residual is relitively noisy with a single point going above the threshold of 5. Because of this many flat curves are fit to lower this single value. The optimum curve fit to the data is a flat line at just above zero so many itteration may go by before the residual drops down enough. Also considering the combined optimization that occurs with each curve each itterative curve fit takes longer and longer.\n",
    " \n",
    "Plot:\n",
    "![itterative_curve_fitting_runtime](images/itterative_curve_fitting_runtime.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8/19\n",
    "\n",
    "Full spectra optimization of curve parameters has proven to be unsuccessful with various optimizations. When peak positions and amplitudes are fixed and the alpha gamma parameters are limited the full optimization does not seem to terminate. This indicates that blocking may be necesary to localise peaks and filter noise through curve fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8/20\n",
    "\n",
    "Iterative curve fitting adjusted to take a cuttoff for maximum number of fitted curves. Guess curve fit adjusted to fit curves more effectively.\n",
    "\n",
    "p0 = [np.max(Y)/100,cen,.01,.01,0]<br>\n",
    "bounds = ([0,cen-B,0,0,0],[np.max(Y),cen+B,2,2,np.amax(Y)])<br>\n",
    "params,_ = curve_fit(voigt,X,Y,p0=p0,bounds=bounds,maxfev=2000)<br>\n",
    "\n",
    "p0 = [np.max(Y)/100,cen,.01,.01,0]<br>\n",
    "bounds = ([0,cen-B,0,0,0],[np.max(Y),cen+B,5,5,np.amax(Y)])<br>\n",
    "params,_ = curve_fit(voigt,X,Y,p0=p0,bounds=bounds,maxfev=2000)<br>\n",
    "\n",
    "p0 = [np.max(Y)/100,cen,2,2,np.amax(Y)/100]<br>\n",
    "bounds = ([0,cen-B,2,2,0],[np.max(Y),cen+B,5,5,np.amax(Y)])<br>\n",
    "params,_ = curve_fit(voigt,X,Y,p0=p0,bounds=bounds,maxfev=2000)<br>\n",
    "\n",
    "    \n",
    "First attempt is to fit a sharp peak. When this fails the range of alpha and gamma values is broadened to allow for wider peaks. When this fails it is likely because there are too many possible parameter values and so the alpha and gamma ranges are again reduced but this time to only wide peaks.\n",
    "\n",
    "Overall this results in better guess curves being fit.\n",
    "Note: Combined optimization is over full alpha and gamma range (0 to 5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8/26\n",
    "(for past week)\n",
    "\n",
    "linkage clustering implemented on top of the iterative peak fitting code. The algorithm is as follows:\n",
    "\n",
    "1) Find most intense peak over wafer.\n",
    "2) Identify peaks in neighboring grid locations that have Q-values in a given range\n",
    "3) Link all peaks that satisfy 2 into a \"layer\" and keep expanding the boarder.\n",
    "\n",
    "The range for the Q-values of neighboring peaks is determined by half the FWHM of the current peak or half the distance to the closest peak in the same pattern as the current peak, whichever happens to be smaller.\n",
    "\n",
    "On a single heatmap this produces the following clustering:\n",
    "![linkage_clustering_heatmap_vertical](images/linkage_clustering_heatmap_vertical.png)\n",
    "![linkage_clustering_heatmap_horizontal](images/linkage_clustering_heatmap_horizontal.png)\n",
    "\n",
    "These are a vertical cut and then a horizontal cut across the wafer, through the middle.\n",
    "\n",
    "The single red x's are peaks that were not linked to anything. These are likely noise.\n",
    "The colors represent peaks that extend across multiple patters and were linked successfully.\n",
    "\n",
    "This linkage can then be extended to multiple dimensions to produce pictures of how the layers look across the wafer.\n",
    "![linkage_clustering_2D](images/linkage_clustering_2D.png)\n",
    "\n",
    "This is a handful of the produced \"layers\" from linking peaks. The transparency represents the peak intensity.\n",
    "It is important to note that the majority of the \"layers\" are fewer than 5 peaks. This means that some filtering may still be necesary.\n",
    "\n",
    "These \"layers\" can then be plotted in 3D. The vertical axis is the Q-axis and the other two are the spacial dimensions on the wafer.\n",
    "![linkage_clustering_3D](images/linkage_clustering_3D.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8/27\n",
    "linkage clustering used to generate cluster plot. 1D diffraction data is reduced to peak space (from linkage clustering) and then PCA is performed to produce clusters.\n",
    "\n",
    "The following clustering is produced\n",
    "![linkage_cluster_plot_2](images/linkage_cluster_plot_2.png)\n",
    "![linkage_cluster_plot_3](images/linkage_cluster_plot_3.png)\n",
    "![linkage_cluster_plot_5](images/linkage_cluster_plot_5.png)\n",
    "![linkage_cluster_plot_8](images/linkage_cluster_plot_8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8/28\n",
    "\n",
    "Split Pseudo Voigt Profile fitting implemented with some results. The primary issue is the increased complexity (2 additional parameters) which lead to increased computation time and convergence failure.\n",
    "\n",
    "Below is the fit for one of the diffraction patters that could be fit successfully. The algorithm was limited to 3 curves in a block. This however was not always reached because of failure to fit a secondary guess curve.\n",
    "\n",
    "![Split_Voigt_TiNiSn_500_C_0052](images/Split_Voigt_TiNiSn_500_C_0052.png)\n",
    "\n",
    "You may notice that in blocks where a second peak would normally be fit with a voigt profile a split-voigt fails to optimize. This may be fixed with a fitting algorithm that puts more weight on the middle of the profile and less on the tails (a custom loss function for the optimization).\n",
    "\n",
    "Another issue is how some peaks are fit:\n",
    "\n",
    "![Split_Voigt_TiNiSn_500C_0021](images/Split_Voigt_TiNiSn_500C_0021.png)\n",
    "\n",
    "Note: the right most block where the main peak is akwardly fit by two curves.\n",
    "\n",
    "\n",
    "From clustering above the PCA components were extracted to produce the following visuals.\n",
    "\n",
    "![components](images/linkage_clustering_PCA_comp/comp_0.png)\n",
    "![components](images/linkage_clustering_PCA_comp/comp_1.png)\n",
    "![components](images/linkage_clustering_PCA_comp/comp_2.png)\n",
    "![components](images/linkage_clustering_PCA_comp/comp_3.png)\n",
    "![components](images/linkage_clustering_PCA_comp/comp_4.png)\n",
    "![components](images/linkage_clustering_PCA_comp/comp_5.png)\n",
    "![components](images/linkage_clustering_PCA_comp/comp_6.png)\n",
    "\n",
    "\n",
    "Note that only the first few components look like they may represent phases while the rest seems to be noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8/29\n",
    "\n",
    "Voigt fitting rerun with a noise level of 10 (instead of the original 5) to see the effect on clustering and the PCA components.\n",
    "\n",
    "The resulting PCA components are similar to the noise 5 case indicating that the underlying issue is not the noise level.\n",
    "\n",
    "![components](images/linkage_clustering_PCA_comp_noise10/comp_0.png)\n",
    "![components](images/linkage_clustering_PCA_comp_noise10/comp_1.png)\n",
    "![components](images/linkage_clustering_PCA_comp_noise10/comp_2.png)\n",
    "![components](images/linkage_clustering_PCA_comp_noise10/comp_3.png)\n",
    "![components](images/linkage_clustering_PCA_comp_noise10/comp_4.png)\n",
    "![components](images/linkage_clustering_PCA_comp_noise10/comp_5.png)\n",
    "![components](images/linkage_clustering_PCA_comp_noise10/comp_6.png)\n",
    "\n",
    "Clustering seems not to be continuous as before and if anything seems more noisy. Significance is unclear.\n",
    "\n",
    "![linkage_clustering_plot_2_noise10](images/linkage_clustering_plot_2_noise10.png)\n",
    "![linkage_clustering_plot_3_noise10](images/linkage_clustering_plot_3_noise10.png)\n",
    "![linkage_clustering_plot_5_noise10](images/linkage_clustering_plot_5_noise10.png)\n",
    "![linkage_clustering_plot_8_noise10](images/linkage_clustering_plot_8_noise10.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
